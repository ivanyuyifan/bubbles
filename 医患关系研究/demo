from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.chrome.options import Options as ChromeOptions
from webdriver_manager.chrome import ChromeDriverManager # 自动管理 ChromeDriver
from bs4 import BeautifulSoup
import time
import random
import csv
import re

# --- 配置参数 ---
KEYWORDS = ["医患关系"]
# START_DATE 和 END_DATE 的精确筛选在微博搜索中比较复杂，
# Selenium 版本中，我们可以尝试导航到高级搜索页面或手动在搜索框输入筛选条件
# 但这里为了简化，我们主要通过关键词和页数控制
MAX_PAGES_PER_KEYWORD = 3 # 每个关键词爬取页数，减少以测试
OUTPUT_CSV_FILE = "weibo_data_selenium.csv"

# --- CSV 文件初始化 ---
def init_csv(filename):
    with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['keyword', 'user_name', 'post_time_raw', 'post_time_formatted', 'post_content', 'source_url'])

# --- 时间格式化函数 (与之前版本相同，可能需要根据实际情况调整) ---
def format_post_time(time_str):
    if not time_str: return ""
    if "分钟前" in time_str or "秒前" in time_str or "刚刚" in time_str:
        return time.strftime("%Y-%m-%d %H:%M", time.localtime()) # 粗略
    elif "今天" in time_str:
        return time.strftime("%Y-%m-%d", time.localtime()) + " " + time_str.split(" ")[-1]
    elif re.match(r"\d{1,2}月\d{1,2}日", time_str): # 例如 "05月25日" 或 "5月25日"
        current_year = time.strftime("%Y", time.localtime())
        parts = re.findall(r'\d+', time_str)
        return f"{current_year}-{int(parts[0]):02d}-{int(parts[1]):02d}"
    elif re.match(r"\d{4}年\d{1,2}月\d{1,2}日", time_str): # 例如 "2023年05月25日"
         parts = re.findall(r'\d+', time_str)
         return f"{parts[0]}-{int(parts[1]):02d}-{int(parts[2]):02d}"
    elif re.match(r"\d{4}-\d{2}-\d{2}", time_str): # 例如 "2023-05-25" (通常带时间)
        return time_str.split(" ")[0] # 只取日期部分
    # 如果是 "MM-DD HH:mm" 或 "YYYY-MM-DD HH:mm" 这种格式，可以直接用或微调
    # 例如：2023-01-15 10:30
    datetime_match = re.search(r"(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2})", time_str)
    if datetime_match:
        return datetime_match.group(1)
    # 例如: 01-15 10:30 (当年)
    month_day_time_match = re.search(r"(\d{2}-\d{2}\s\d{2}:\d{2})", time_str)
    if month_day_time_match:
        current_year = time.strftime("%Y", time.localtime())
        return f"{current_year}-{month_day_time_match.group(1)}"
    return time_str


# --- 解析单条微博 (使用 BeautifulSoup 解析从 Selenium 获取的 HTML 片段) ---
def parse_weibo_post_selenium(post_element_html, keyword, driver):
    """
    解析单个微博 HTML 片段，提取所需信息。
    这里的 post_element_html 是 Selenium WebElement 的 .get_attribute('outerHTML')
    """
    soup = BeautifulSoup(post_element_html, 'html.parser')
    try:
        # 用户名
        # 微博的class名经常会变，比如 feed_list_content -> content, name -> head-name
        user_name_tag = soup.select_one('.head-info .from a.name, .header .name, .content .info .name, .avator+div .name') # 尝试多种可能的选择器
        user_name = user_name_tag.text.strip() if user_name_tag else "未知用户"

        # 发布时间 (这个是最不稳定的，微博结构经常改动)
        # 尝试 .head-info .head-info_time, .from a, .footer .time 等
        time_tag_candidates = [
            soup.select_one('.head-info_time'),
            soup.select_one('.from a:first-child'),
            soup.select_one('.footer .time'),
            soup.select_one('.content .from span:first-child'), # 新版可能结构
            soup.select_one('div.from span:first-child a') # 又一种可能
        ]
        post_time_raw = ""
        post_url = ""

        for time_tag in time_tag_candidates:
            if time_tag:
                post_time_raw = time_tag.text.strip()
                if time_tag.name == 'a' and 'href' in time_tag.attrs:
                    href = time_tag['href']
                    if href.startswith('//weibo.com'):
                        post_url = "https:" + href
                    elif href.startswith('/'): # 相对路径, 可能需要拼接
                        # e.g., /1234567890/OabcDEfgh
                        if re.match(r"/\d+/[A-Za-z0-9]+", href):
                            post_url = "https://weibo.com" + href
                if post_time_raw: # 找到一个就用
                    break
        
        if not post_url and soup.select_one('a.head-info_time[href]'): # 再尝试从时间链接获取
             post_url = "https://weibo.com" + soup.select_one('a.head-info_time[href]')['href']


        post_time_formatted = format_post_time(post_time_raw)

        # 正文内容
        # class="detail_text_ᴀLLS5" "detail_wbtext_04l0z" "detail_longtext_s4TKh" 经常变
        # 用更通用的，比如 data-longtext="true" 的父级, 或者 id 以 `detail鰯` 开头的
        content_tag = soup.select_one('div[id^="detail"], div[class*="detail_wbtext"], div[class*="detail_text"]')
        post_content = ""
        if content_tag:
            # 尝试获取所有直接文本节点和子节点文本，排除特定不想要的标签
            text_parts = []
            for elem in content_tag.descendants:
                if isinstance(elem, str):
                    text_parts.append(elem.strip())
                elif elem.name == 'br':
                    text_parts.append(' ') # 换行替换为空格
                elif elem.name == 'a' and '查看图片' in elem.text: # 忽略查看图片链接
                    continue
                elif elem.name == 'img': # 忽略图片 alt 或 emoji
                    if elem.get('alt'):
                         text_parts.append(elem.get('alt')) # 如果是 emoji 的 alt
            post_content = "".join(text_parts).strip().replace('\n', ' ').replace('\u200b', '')
            post_content = re.sub(r'\s+', ' ', post_content) # 合并多余空格
            post_content = re.sub(r'收起图片.*$', '', post_content) # 去掉 "收起图片" 及之后的内容
            post_content = re.sub(r'展开c$', '', post_content) # 去掉可能的 "展开c"
            post_content = re.sub(r'网页链接', '', post_content) # 去掉 "网页链接" 文本
        else:
            post_content = "内容获取失败"
        
        # 如果内容为空，尝试另一种可能（旧版微博）
        if not post_content or post_content == "内容获取失败":
            legacy_content_tag = soup.select_one('.content .txt')
            if legacy_content_tag:
                # 移除 "展开全文" 等
                for el in legacy_content_tag.select('a[action-type="fl_unfold"], a.k_unfold'):
                    el.decompose()
                post_content = legacy_content_tag.text.strip().replace('\n', ' ').replace('\u200b', '')
                post_content = re.sub(r'\s+', ' ', post_content)


        return {
            'keyword': keyword,
            'user_name': user_name,
            'post_time_raw': post_time_raw,
            'post_time_formatted': post_time_formatted,
            'post_content': post_content,
            'source_url': post_url
        }
    except Exception as e:
        print(f"解析单条微博HTML时发生错误: {e}")
        return None

# --- 主函数 ---
def main():
    init_csv(OUTPUT_CSV_FILE)
    all_posts_count = 0

    # --- Selenium WebDriver 初始化 ---
    print("正在初始化浏览器驱动...")
    options = ChromeOptions()
    # options.add_argument("--headless")  # 如果不需要看浏览器界面，可以开启无头模式，但登录时需要关闭
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("user-agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'")
    options.add_experimental_option('excludeSwitches', ['enable-automation']) # 避免被检测
    options.add_experimental_option('useAutomationExtension', False)

    try:
        # 使用 webdriver_manager 自动下载和管理 ChromeDriver
        service = ChromeService(executable_path=ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
    except Exception as e:
        print(f"初始化 WebDriver 失败: {e}")
        print("请确保 Chrome 浏览器已安装，并且 ChromeDriver 与您的 Chrome 版本兼容。")
        print("您可以从 https://chromedriver.chromium.org/downloads 下载合适的 ChromeDriver，")
        print("并将其路径配置到系统 PATH，或者直接在代码中指定 executable_path。")
        return

    # --- 手动登录 ---
    print("\n" + "="*50)
    print("浏览器已启动。请在新打开的 Chrome 浏览器窗口中手动登录您的微博账号。")
    print("登录成功后，请确保您能正常浏览微博内容。")
    driver.get("https://weibo.com/") # 打开微博首页，方便登录
    # driver.get("https://s.weibo.com/") # 或者直接打开搜索页
    input("登录完成后，请在本控制台按 Enter键 继续爬虫程序...")
    print("="*50 + "\n")
    print("已确认登录，开始爬取数据...")

    for keyword in KEYWORDS:
        print(f"开始爬取关键词: {keyword}")
        for page in range(1, MAX_PAGES_PER_KEYWORD + 1):
            print(f"  正在爬取关键词 '{keyword}' 的第 {page} 页...")
            # 微博的搜索URL结构 (q=关键词, page=页码)
            # 对于更精确的筛选 (如时间)，URL会更复杂，可能需要导航到高级搜索页面操作
            search_url = f"https://s.weibo.com/weibo?q={keyword}&page={page}"
            try:
                driver.get(search_url)
                # 等待页面加载一些特征元素，比如微博卡片
                # 这个等待时间需要根据您的网络情况调整，或者使用更智能的显式等待
                time.sleep(random.uniform(5, 8)) # 等待AJAX加载，微博搜索结果是动态加载的

                # 滚动页面以加载更多动态内容 (微博搜索页通常需要滚动)
                print("  尝试向下滚动页面加载内容...")
                last_height = driver.execute_script("return document.body.scrollHeight")
                for _ in range(3): # 滚动几次
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(random.uniform(2,4))
                    new_height = driver.execute_script("return document.body.scrollHeight")
                    if new_height == last_height:
                        break
                    last_height = new_height

                page_source = driver.page_source
                soup = BeautifulSoup(page_source, 'html.parser')

                # 定位到包含微博内容的元素
                # 新版微博 class 名称: .vue-recycle-scroller__item-view (这是滚动列表的单个项目)
                # card-wrap 可能仍然在某些情况下有效，或者 .card .card-feed
                # 需要根据实际情况调整
                post_elements_candidates = [
                    soup.select('div.vue-recycle-scroller__item-view article.card-wrap'), # 新版PC端搜索结果结构
                    soup.select('div.card-wrap[action-type="feed_list_item"]'), # 旧版或某些情况
                    soup.select('div.card div.card-feed'), # 另一种可能结构
                    soup.select('div.wb-item-wrap') # m.weibo.cn 风格，但s.weibo.com 也可能出现
                ]
                
                post_elements = []
                for candidate_selector in post_elements_candidates:
                    if candidate_selector:
                        post_elements = candidate_selector
                        if post_elements: # 找到一个就不再尝试其他
                            print(f"  使用选择器找到了 {len(post_elements)} 个可能的微博条目。")
                            break
                
                if not post_elements:
                    print(f"  第 {page} 页未找到微博内容或页面结构已改变。请检查CSS选择器。")
                    # 可以考虑在这里保存截图方便调试
                    # driver.save_screenshot(f"debug_page_{keyword}_{page}.png")
                    # print(f"  已保存截图: debug_page_{keyword}_{page}.png")
                    if "login.sina.com.cn" in page_source or "passport.weibo.com" in page_source:
                        print("  警告：可能被重定向到登录页，您的登录会话可能已失效。")
                    elif "访问受限" in page_source or "验证码" in page_source:
                        print("  警告：访问可能受限或需要验证码。")
                    break

                page_posts_count = 0
                with open(OUTPUT_CSV_FILE, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    for post_el_soup in post_elements: # post_el_soup 是 BeautifulSoup 的 Tag 对象
                        # 将BeautifulSoup的Tag对象转换为HTML字符串给解析函数
                        post_html_content = str(post_el_soup)
                        parsed_data = parse_weibo_post_selenium(post_html_content, keyword, driver)

                        if parsed_data and parsed_data['post_content'] and parsed_data['post_content'] != "内容获取失败":
                            # 过滤掉内容过短的（可能是广告或无效信息）
                            if len(parsed_data['post_content']) < 10: # 假设有效内容至少10个字
                                # print(f"  过滤短内容: {parsed_data['post_content'][:20]}...")
                                continue
                            writer.writerow([
                                parsed_data['keyword'],
                                parsed_data['user_name'],
                                parsed_data['post_time_raw'],
                                parsed_data['post_time_formatted'],
                                parsed_data['post_content'],
                                parsed_data['source_url']
                            ])
                            page_posts_count += 1
                            all_posts_count += 1

                print(f"  第 {page} 页成功解析并保存 {page_posts_count} 条微博。")

                # 随机延时
                sleep_duration = random.uniform(4, 8)
                print(f"  暂停 {sleep_duration:.2f} 秒...")
                time.sleep(sleep_duration)

            except Exception as e:
                print(f"处理关键词 '{keyword}' 第 {page} 页时发生错误: {e}")
                # driver.save_screenshot(f"error_page_{keyword}_{page}.png")
                # print(f"  已保存错误截图: error_page_{keyword}_{page}.png")
                # break # 出现错误，停止当前关键词的爬取
        print(f"关键词 '{keyword}' 爬取完毕。\n")

    print(f"所有关键词爬取完毕，总共爬取 {all_posts_count} 条微博。数据已保存到 {OUTPUT_CSV_FILE}")
    driver.quit()
    print("浏览器已关闭。")

if __name__ == '__main__':
    main()